{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Dinov2PreTrainedModel, Dinov2Model\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import warnings\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Palette images with Transparency expressed in bytes should be converted to RGBA images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sub-elements from txt file\n",
    "def load_sub_elements(txt_file):\n",
    "    sub_elements = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('.png ')\n",
    "            if len(parts) >= 2:\n",
    "                filename = parts[0]\n",
    "                # print(filename)\n",
    "                category = parts[1]\n",
    "                sub_elements.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"category\": category\n",
    "                })\n",
    "                # print(sub_elements)\n",
    "    return sub_elements\n",
    "\n",
    "# Load image database from JSON file\n",
    "def load_image_database(json_file):\n",
    "    image_db = []\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for item in data:\n",
    "            num_categories = int(len(item)/2-1)\n",
    "            # print(num_categories)\n",
    "            if len(item) >= 2:\n",
    "                filename = item[1]\n",
    "                categories = item[num_categories+2:]\n",
    "                image_db.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"categories\": categories\n",
    "                })\n",
    "    return image_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet dataset building\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, sub_elements, image_db, sub_elements_dir, images_dir, transform=None):\n",
    "        self.sub_elements = sub_elements\n",
    "        self.image_db = image_db\n",
    "        self.sub_elements_dir = sub_elements_dir\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.category_to_images = {}\n",
    "        for img in self.image_db:\n",
    "            for cat in img[\"categories\"]:\n",
    "                if cat not in self.category_to_images:\n",
    "                    self.category_to_images[cat] = []\n",
    "                self.category_to_images[cat].append(img)\n",
    "\n",
    "        self.category_exclude_images = {}\n",
    "        all_image_indices = set(range(len(self.image_db)))\n",
    "        for cat in self.category_to_images:\n",
    "            cat_image_indices = {i for i, img in enumerate(self.image_db) if cat in img[\"categories\"]}\n",
    "            self.category_exclude_images[cat] = list(all_image_indices - cat_image_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sub_elements)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sub_element = self.sub_elements[idx]\n",
    "        sub_element_filename = sub_element[\"filename\"]\n",
    "        sub_element_category = sub_element[\"category\"]\n",
    "\n",
    "        sub_element_path = sub_element_filename + str('.png')\n",
    "        try:\n",
    "            sub_element_image = Image.open(sub_element_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {sub_element_path}: {e}\")\n",
    "            sub_element_image = Image.new('RGB', (224, 224))\n",
    "        \n",
    "        sub_element_image = np.array(sub_element_image)\n",
    "        positive_images = self.category_to_images.get(sub_element_category, [])\n",
    "        if not positive_images:\n",
    "            print(f\"Warning: No positive images found for category {sub_element_category} in image_db\")\n",
    "            positive_img = self.image_db[np.random.randint(0, len(self.image_db))]\n",
    "        else:\n",
    "            positive_img = np.random.choice(positive_images)\n",
    "            assert sub_element_category in positive_img[\"categories\"], \"Selected positive image does not contain target category\"\n",
    "        \n",
    "        positive_filename = positive_img[\"filename\"]\n",
    "        positive_path = os.path.join(self.images_dir, f\"{positive_filename}.png\")\n",
    "        try:\n",
    "            positive_image = Image.open(positive_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {positive_path}: {e}\")\n",
    "            positive_image = Image.new('RGB', (224, 224))\n",
    "        \n",
    "        positive_image = np.array(positive_image)\n",
    "\n",
    "        negative_images = self.category_exclude_images.get(sub_element_category, [])\n",
    "        if not negative_images:\n",
    "            print(f\"Warning: No negative images found for category {sub_element_category} in image_db\")\n",
    "            negative_img = self.image_db[np.random.randint(0, len(self.image_db))]\n",
    "        else:\n",
    "            negative_img = self.image_db[np.random.choice(negative_images)]\n",
    "            assert sub_element_category not in negative_img[\"categories\"], \"Selected negative image contains target category\"\n",
    "\n",
    "        negative_filename = negative_img[\"filename\"]\n",
    "        negative_path = os.path.join(self.images_dir, f\"{negative_filename}.png\")\n",
    "        try:\n",
    "            negative_image = Image.open(negative_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {negative_path}: {e}\")\n",
    "            negative_image = Image.new('RGB', (224, 224))\n",
    "        \n",
    "        negative_image = np.array(negative_image)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed_anchor = self.transform(image=sub_element_image)\n",
    "            anchor = transformed_anchor[\"image\"]\n",
    "\n",
    "            transformed_positive = self.transform(image=positive_image)\n",
    "            positive = transformed_positive[\"image\"]\n",
    "\n",
    "            transformed_negative = self.transform(image=negative_image)\n",
    "            negative = transformed_negative[\"image\"]\n",
    "\n",
    "        anchor = torch.tensor(anchor).permute(2, 0, 1).float()\n",
    "        positive = torch.tensor(positive).permute(2, 0, 1).float()\n",
    "        negative = torch.tensor(negative).permute(2, 0, 1).float()\n",
    "        \n",
    "        return {\n",
    "            \"anchor\": anchor,\n",
    "            \"positive\": positive,\n",
    "            \"negative\": negative,\n",
    "            \"anchor_path\": sub_element_path,\n",
    "            \"positive_path\": positive_path,\n",
    "            \"negative_path\": negative_path,\n",
    "            \"category\": sub_element_category,\n",
    "            \"positive_categories\": positive_img[\"categories\"],\n",
    "            \"negative_categories\": negative_img[\"categories\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction model\n",
    "class Dinov2FeatureExtractor(Dinov2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dinov2 = Dinov2Model(config)\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "        )\n",
    "        \n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self._unfreeze_dinov2_layers(2)\n",
    "\n",
    "        for param in self.projection_head.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def _unfreeze_dinov2_layers(self, unfreeze_layers):\n",
    "        try:\n",
    "            total_blocks = len(self.dinov2.encoder.layer)\n",
    "            layers_to_unfreeze = max(0, total_blocks - unfreeze_layers)\n",
    "            \n",
    "            print(f\"Unfreeze the last {unfreeze_layers} Transformer blocks ({layers_to_unfreeze}-{total_blocks-1})\")\n",
    " \n",
    "            for i in range(layers_to_unfreeze, total_blocks):\n",
    "                for param in self.dinov2.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"Unfreeze block {i}\")\n",
    "\n",
    "            for param in self.dinov2.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Unfreeze layernorm layer\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during unfreezing: {e}\")\n",
    "            print(\"Only train the projection head\")\n",
    "    \n",
    "    def forward(self, pixel_values, output_hidden_states=False, output_attentions=False):\n",
    "        outputs = self.dinov2(\n",
    "            pixel_values,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "        features = self.projection_head(cls_token)  # [batch_size, 256]\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'last_hidden_state': outputs.last_hidden_state,\n",
    "            'hidden_states': outputs.hidden_states,\n",
    "            'attentions': outputs.attentions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triple loss function\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.2):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.distance_fn = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_sim = self.distance_fn(anchor, positive)\n",
    "        neg_sim = self.distance_fn(anchor, negative)\n",
    "\n",
    "        loss = torch.mean(torch.clamp(self.margin - pos_sim + neg_sim, min=0.0))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "   # Data path\n",
    "    sub_elements_txt = '/Dataset/train_sub.txt'  # Path and category of sub elements training dataset\n",
    "    val_sub_txt = '/Dataset/val_sub.txt' # Path and category of sub elements validating dataset\n",
    "    image_db_json = '/Dataset/Train_split.json'  # Image database for training\n",
    "    val_image_db = '/Dataset/Validation_split.json' # Image database for validating\n",
    "    sub_elements_dir = '/Dataset/element img'  # Sub-element images directory\n",
    "    images_dir = '/Dataset/SimulatedPrintedFabrics-17k/train/img/images'  # Images for training\n",
    "    val_dir = '/Dataset/SimulatedPrintedFabrics-17k/validation/img/images' # Images for validating\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading sub-elements...\")\n",
    "    sub_elements = load_sub_elements(sub_elements_txt)\n",
    "    val_sub = load_sub_elements(val_sub_txt)\n",
    "    print(f\"Loaded {len(sub_elements)} sub-elements\")\n",
    "    \n",
    "    print(\"Loading image database...\")\n",
    "    image_db = load_image_database(image_db_json)\n",
    "    val_db = load_image_database(val_image_db)\n",
    "    print(f\"Loaded {len(image_db)} images\")\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_size = len(sub_elements)\n",
    "    val_size = len(val_sub)\n",
    "    print(train_size, val_size)\n",
    "    train_sub_elements = sub_elements\n",
    "    val_sub_elements = val_sub\n",
    "    \n",
    "    train_dataset = TripletDataset(train_sub_elements, image_db, sub_elements_dir, images_dir, train_transform)\n",
    "    val_dataset = TripletDataset(val_sub_elements, val_db, sub_elements_dir, val_dir, val_transform)\n",
    "    \n",
    "    # Dataloader creation\n",
    "    def collate_fn(batch):\n",
    "        anchor_batch = torch.stack([item[\"anchor\"] for item in batch])\n",
    "        positive_batch = torch.stack([item[\"positive\"] for item in batch])\n",
    "        negative_batch = torch.stack([item[\"negative\"] for item in batch])\n",
    "        \n",
    "        return {\n",
    "            \"anchor\": anchor_batch,\n",
    "            \"positive\": positive_batch,\n",
    "            \"negative\": negative_batch,\n",
    "            \"anchor_paths\": [item[\"anchor_path\"] for item in batch],\n",
    "            \"positive_paths\": [item[\"positive_path\"] for item in batch],\n",
    "            \"negative_paths\": [item[\"negative_path\"] for item in batch],\n",
    "            \"categories\": [item[\"category\"] for item in batch]\n",
    "        }\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=6, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "    \n",
    "    # Model initialization\n",
    "    model = Dinov2FeatureExtractor.from_pretrained(\"/home/mayunjiao/MYJ/mihui/mihui/Pre-trained/dinov2-pytorch-base-v1\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Definition of loss functions\n",
    "    triplet_criterion = TripletLoss(margin=0.2)\n",
    "\n",
    "    lr = 1e-5\n",
    "    params = [\n",
    "        {'params': [p for n, p in model.named_parameters() if 'dinov2' in n and p.requires_grad], 'lr': lr/10},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'projection_head' in n], 'lr': lr}\n",
    "    ]\n",
    "    feature_optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=1e-4)\n",
    "    feature_scheduler = CosineAnnealingLR(feature_optimizer, T_max=80, eta_min=lr/100)\n",
    "    \n",
    "    # Training function\n",
    "    def train_one_epoch(feature_extractor, dataloader, triplet_criterion, feature_optimizer, device):\n",
    "        feature_extractor.train()\n",
    "        total_triplet_loss = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            anchor = batch[\"anchor\"].to(device)\n",
    "            positive = batch[\"positive\"].to(device)\n",
    "            negative = batch[\"negative\"].to(device)\n",
    "            \n",
    "            feature_optimizer.zero_grad()\n",
    "\n",
    "            anchor_outputs = feature_extractor(anchor)\n",
    "            positive_outputs = feature_extractor(positive)\n",
    "            negative_outputs = feature_extractor(negative)\n",
    "            \n",
    "            anchor_features = anchor_outputs[\"features\"]\n",
    "            positive_features = positive_outputs[\"features\"]\n",
    "            negative_features = negative_outputs[\"features\"]\n",
    "\n",
    "            triplet_loss = triplet_criterion(anchor_features, positive_features, negative_features)           \n",
    "\n",
    "            triplet_loss.backward()\n",
    "            feature_optimizer.step()\n",
    "            \n",
    "            total_triplet_loss += triplet_loss.item()\n",
    "            \n",
    "        return total_triplet_loss / len(dataloader)\n",
    "    \n",
    "    # Evaluation function\n",
    "    def evaluate(feature_extractor, dataloader, triplet_criterion, device):\n",
    "        feature_extractor.eval()\n",
    "        total_triplet_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                anchor = batch[\"anchor\"].to(device)\n",
    "                positive = batch[\"positive\"].to(device)\n",
    "                negative = batch[\"negative\"].to(device)\n",
    "\n",
    "                anchor_features = feature_extractor(anchor)[\"features\"]\n",
    "                positive_features = feature_extractor(positive)[\"features\"]\n",
    "                negative_features = feature_extractor(negative)[\"features\"]\n",
    "\n",
    "                triplet_loss = triplet_criterion(anchor_features, positive_features, negative_features)\n",
    "                total_triplet_loss += triplet_loss.item()\n",
    "        \n",
    "        return total_triplet_loss / len(dataloader)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    print('Starting training...')\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_triplet_loss = train_one_epoch(\n",
    "            model, train_dataloader, \n",
    "            triplet_criterion,\n",
    "            feature_optimizer, device\n",
    "        )\n",
    "        \n",
    "        val_triplet_loss = evaluate(\n",
    "            model, val_dataloader, \n",
    "            triplet_criterion, device\n",
    "        )\n",
    "\n",
    "        feature_scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: Triplet={train_triplet_loss:.4f}\")\n",
    "        print(f\"Val Loss:   Triplet={val_triplet_loss:.4f}\")\n",
    "        \n",
    "        # Model checkpoint saving\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'feature_extractor_state_dict': model.state_dict(),\n",
    "                'feature_optimizer_state_dict': feature_optimizer.state_dict(),\n",
    "                'train_triplet_loss': train_triplet_loss,\n",
    "                'val_triplet_loss': val_triplet_loss,\n",
    "            }, f'Weight_Path/dinov2_finetuning2_epoch_{epoch+1}.pth')\n",
    "    \n",
    "    # Feature database construction\n",
    "    def build_feature_database(feature_extractor, dataloader, device):\n",
    "        feature_db = {}\n",
    "        feature_extractor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images = batch[\"positive\"].to(device)\n",
    "                paths = batch[\"positive_paths\"]\n",
    "\n",
    "                features = feature_extractor(images)[\"features\"].cpu().numpy()\n",
    "\n",
    "                for i, path in enumerate(paths):\n",
    "                    feature_db[path] = features[i]\n",
    "        \n",
    "        return feature_db\n",
    "    \n",
    "    # Retrieval\n",
    "    def retrieve_images(query_sub_element, feature_extractor, feature_db, top_k=5):\n",
    "        query_np = np.array(query_sub_element)\n",
    "        transformed = val_transform(image=query_np)\n",
    "        query_tensor = torch.tensor(transformed[\"image\"]).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # query feature extraction\n",
    "        feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            query_features = feature_extractor(query_tensor)[\"features\"].squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Initial retrieval and ranking\n",
    "        similarities = []\n",
    "        for img_path, features in feature_db.items():\n",
    "            sim = np.dot(query_features, features) / (np.linalg.norm(query_features) * np.linalg.norm(features))\n",
    "            similarities.append((img_path, sim))\n",
    "        \n",
    "        # Sort by similarity and get top_k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    print(\"Building feature database...\")\n",
    "    feature_db = build_feature_database(model, val_dataloader, device)\n",
    "    print(f\"Feature database built with {len(feature_db)} entries\")\n",
    "    \n",
    "    # Random selection of a query sub-element for retrieval demonstration\n",
    "    if val_dataset:\n",
    "        sample_idx = np.random.randint(0, len(val_dataset))\n",
    "        sample = val_dataset[sample_idx]\n",
    "        query_image = Image.open(sample[\"anchor_path\"]).convert('RGB')\n",
    "        \n",
    "        print(f\"\\nPerforming retrieval with query: {sample['anchor_path']} (Category: {sample['category']})\")\n",
    "        results = retrieve_images(query_image, model, feature_db, top_k=3)\n",
    "        \n",
    "        print(\"\\nTop retrieval results after reranking:\")\n",
    "        for i, (path, score) in enumerate(results):\n",
    "            print(f\"{i+1}. {path} (ReRank Score: {score:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myj_jupyterlab",
   "language": "python",
   "name": "myj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
