{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from transformers import Dinov2PreTrainedModel, Dinov2Model\n",
    "import csv\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data processing\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "# Dinov2\n",
    "class Dinov2FeatureExtractor(Dinov2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dinov2 = Dinov2Model(config)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "        )\n",
    "\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self._unfreeze_dinov2_layers(2)\n",
    "        for param in self.projection_head.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def _unfreeze_dinov2_layers(self, unfreeze_layers):\n",
    "        try:\n",
    "            total_blocks = len(self.dinov2.encoder.layer)\n",
    "            layers_to_unfreeze = max(0, total_blocks - unfreeze_layers)\n",
    "            \n",
    "            print(f\"Unfreeze the last {unfreeze_layers} Transformer blocks ({layers_to_unfreeze}-{total_blocks-1})\")\n",
    "\n",
    "            for i in range(layers_to_unfreeze, total_blocks):\n",
    "                for param in self.dinov2.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"Unfreeze block {i}\")\n",
    "\n",
    "            for param in self.dinov2.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Unfreeze layernorm layer\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during unfreezing: {e}\")\n",
    "            print(\"Only train the projection head\")\n",
    "    \n",
    "    def forward(self, pixel_values, output_hidden_states=False, output_attentions=False,return_attentions=False):\n",
    "        outputs = self.dinov2(\n",
    "            pixel_values,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "        features = self.projection_head(cls_token)  # [batch_size, 256]\n",
    "        if return_attentions:\n",
    "            return features, outputs.last_hidden_state, outputs.hidden_states, outputs.attentions\n",
    "        else:\n",
    "            # return query_feat, target_feat, align_feat\n",
    "            return {\n",
    "                'features': features,\n",
    "                'last_hidden_state': outputs.last_hidden_state,\n",
    "                'hidden_states': outputs.hidden_states,\n",
    "                'attentions': outputs.attentions\n",
    "            }\n",
    "\n",
    "# Query-Guided Attention Module\n",
    "class QueryGuidedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, query_global, target_spatial):\n",
    "        \"\"\"\n",
    "        query_global: [batch_size, 1, hidden_size] sub-element features\n",
    "        target_spatial: [batch_size, seq_len, hidden_size] image features \n",
    "        \"\"\"\n",
    "        context, attn_weights = self.multihead_attn(\n",
    "            query=query_global,\n",
    "            key=target_spatial,\n",
    "            value=target_spatial,\n",
    "            need_weights=True\n",
    "        )\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "class AttentionFeatureExtractor(Dinov2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dinov2 = Dinov2Model(config)\n",
    "        self.cross_attention = QueryGuidedAttention(hidden_size=config.hidden_size, num_heads=8, dropout=0.1)\n",
    "\n",
    "        self.query_projection = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            L2Norm()\n",
    "        )\n",
    "        \n",
    "        self.target_projection = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            L2Norm()\n",
    "        )\n",
    "\n",
    "        self.attention_align = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "            L2Norm()\n",
    "        )\n",
    "\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self._unfreeze_dinov2_layers(2)\n",
    "\n",
    "        for param in self.query_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.target_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.attention_align.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.cross_attention.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def _unfreeze_dinov2_layers(self, unfreeze_layers):\n",
    "        try:\n",
    "            total_blocks = len(self.dinov2.encoder.layer)\n",
    "            layers_to_unfreeze = max(0, total_blocks - unfreeze_layers)\n",
    "            \n",
    "            print(f\"Unfreeze the last {unfreeze_layers} Transformer blocks ({layers_to_unfreeze}-{total_blocks-1})\")\n",
    " \n",
    "            for i in range(layers_to_unfreeze, total_blocks):\n",
    "                for param in self.dinov2.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"Unfreeze block {i}\")\n",
    "\n",
    "            for param in self.dinov2.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Unfreeze layernorm layer\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during unfreezing: {e}\")\n",
    "            print(\"Only train the projection head\")\n",
    "    \n",
    "    def forward(self, query_images, target_images, output_hidden_states=False, output_attentions=False, is_train=True, return_attentions=False):\n",
    "        query_outputs = self.dinov2(\n",
    "            query_images,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "        query_global = query_outputs.last_hidden_state[:, :1, :] # Sub-element takes the CLS token as feature\n",
    "\n",
    "        if is_train and target_images is not None:\n",
    "            target_outputs = self.dinov2(target_images)\n",
    "            target_spatial = target_outputs.last_hidden_state[:, 1:, :] # Target image takes the patch tokens as features\n",
    "\n",
    "            context, attn_weights = self.cross_attention(\n",
    "                query_global, \n",
    "                target_spatial\n",
    "            ) # query: query_global, key/value: target_spatial\n",
    "\n",
    "            query_feat = self.query_projection(query_global.squeeze(1))\n",
    "            target_feat = self.target_projection(context.squeeze(1))\n",
    "            align_feat = self.attention_align(query_global.squeeze(1)) \n",
    "\n",
    "            if return_attentions:\n",
    "                return query_feat, target_feat, align_feat, attn_weights\n",
    "            else:\n",
    "                return query_feat, target_feat, align_feat\n",
    "        else:\n",
    "            return self.query_projection(query_global.squeeze(1))\n",
    "        \n",
    "# Model loading\n",
    "def load_model(model_config_path, weights_path, device):\n",
    "    feature_extractor  = AttentionFeatureExtractor.from_pretrained(model_config_path)\n",
    "    \n",
    "    # Load weights\n",
    "    try:\n",
    "        checkpoint = torch.load(weights_path, map_location=device)\n",
    "        feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n",
    "        print(f\"Successfully loaded feature extractor weights from {weights_path}\")\n",
    "        print(f\"Model trained for {checkpoint.get('epoch', 'unknown')} epochs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model weights: {e}\")\n",
    "        print(\"Using model with default weights\")\n",
    "    \n",
    "    feature_extractor.to(device)\n",
    "    return feature_extractor\n",
    "\n",
    "# Fine reranking candidate images using cross-attention\n",
    "def perform_cross_attention_reranking(feature_extractor, query_image, candidate_paths, top_k=5):\n",
    "    # Query image preprocessing\n",
    "    query_np = np.array(query_image)\n",
    "    transformed_query = val_transform(image=query_np)\n",
    "    query_tensor = torch.tensor(transformed_query[\"image\"]).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "\n",
    "    similarities = [] # Similarity scores for candidates\n",
    "    \n",
    "    # Process each candidate image\n",
    "    for candidate_path in candidate_paths:\n",
    "        try:\n",
    "            # Load and preprocess candidate image\n",
    "            candidate_img = Image.open(candidate_path).convert('RGB')\n",
    "            candidate_np = np.array(candidate_img)\n",
    "            transformed_candidate = val_transform(image=candidate_np)\n",
    "            candidate_tensor = torch.tensor(transformed_candidate[\"image\"]).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "            \n",
    "            # Feature extraction with cross-attention\n",
    "            feature_extractor.eval()\n",
    "            with torch.no_grad():\n",
    "                query_feat, candidate_feat, _ = feature_extractor(query_tensor, candidate_tensor)\n",
    "                sim = F.cosine_similarity(query_feat, candidate_feat).item()\n",
    "                similarities.append((candidate_path, sim))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing candidate {candidate_path}: {e}\")\n",
    "            similarities.append((candidate_path, -10.0))\n",
    "    \n",
    "    # Rank candidates by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Perform reranking from first retrieval results\n",
    "def perform_reranking_from_first_results(model_config_path, weights_path, query_file_path, \n",
    "                                        first_retrieval_path, top_k=5, output_json_path=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model\n",
    "    feature_extractor = load_model(model_config_path, weights_path, device)\n",
    "    \n",
    "    # Load first retrieval results\n",
    "    try:\n",
    "        with open(first_retrieval_path, 'r') as f:\n",
    "            first_results = json.load(f)\n",
    "        print(f\"Loaded first retrieval results from {first_retrieval_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading first retrieval results: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Get query image paths\n",
    "    try:\n",
    "        with open(query_file_path, 'r') as f:\n",
    "            query_image_paths = [line.strip().split('.png ')[0] + \".png\" for line in f if line.strip()]\n",
    "        print(f\"Found {len(query_image_paths)} query images in {query_file_path}\")\n",
    "    \n",
    "        # Sorting query images\n",
    "        reranking_results = {}\n",
    "        for query_path in query_image_paths:\n",
    "            try:\n",
    "                if not os.path.exists(query_path):\n",
    "                    print(f\"Warning: Query image not found: {query_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Load query image\n",
    "                query_image = Image.open(query_path).convert('RGB')\n",
    "                # print(f\"\\nPerforming retrieval with query: {query_path}\")\n",
    "                \n",
    "                # Get candidate images from first retrieval\n",
    "                candidate_paths = [result[\"image_path\"] for result in first_results.get(query_path, [])]\n",
    "                \n",
    "                if not candidate_paths:\n",
    "                    print(f\"Warning: No candidates found for query {query_path}\")\n",
    "                    continue\n",
    "\n",
    "                results = perform_cross_attention_reranking(\n",
    "                    feature_extractor, query_image, candidate_paths, top_k=top_k\n",
    "                )\n",
    "\n",
    "                # print(f\"Top {top_k} retrieval results:\")\n",
    "                # for i, (path, score) in enumerate(results):\n",
    "                #     print(f\"{i+1}. {path} (Similarity: {score:.4f})\")\n",
    "                \n",
    "                reranking_results[query_path] = results\n",
    "                print(f\"Processed query: {query_path}, found {len(results)} results\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query {query_path}: {e}\")\n",
    "\n",
    "            if output_json_path and reranking_results:\n",
    "                os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "                with open(output_json_path, 'w') as json_file:\n",
    "                    json.dump(reranking_results, json_file)\n",
    "                print(f\"Saved retrieval results to {output_json_path}\")\n",
    "            \n",
    "        return reranking_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading query file: {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_ground_truth(correct_result_file, class_path):\n",
    "    \"\"\"\n",
    "    Load ground truth data from CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    - correct_result_file: Ground truth including target categories\n",
    "    - class_path: Category information of the query images\n",
    "    \n",
    "    Returns:\n",
    "    - Ground truth dictionary: a dictionary where keys are categories and values are sets of correct results\n",
    "    \"\"\"\n",
    "    # Get query image categories\n",
    "    query_classes = {}\n",
    "    with open(class_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row) >= 2:\n",
    "                filename = row[0]\n",
    "                class_name = row[1]\n",
    "                query_classes[filename] = class_name\n",
    "    print(f\"Loaded classes for {len(query_classes)} queries\")\n",
    "\n",
    "    ground_truth = {}\n",
    "    \n",
    "    # Get ground truth results\n",
    "    with open(correct_result_file, 'r') as correct_file:\n",
    "        correct_reader = csv.reader(correct_file)\n",
    "        # Get filenames as key\n",
    "        try:\n",
    "            filenames = next(correct_reader)\n",
    "        except StopIteration:\n",
    "            print(\"Error: Correct results file is empty!\")\n",
    "            return {}\n",
    "        \n",
    "        for row in correct_reader:\n",
    "            for i, value in enumerate(row):\n",
    "                if value.strip():\n",
    "                    if filenames[i] not in ground_truth:\n",
    "                        ground_truth[filenames[i]] = set()\n",
    "                    ground_truth[filenames[i]].add(value.strip())\n",
    "                    # print(class_name, os.path.splitext(value.strip())[0])\n",
    "    # print(ground_truth)\n",
    "    print(f\"Loaded ground truth data for {len(ground_truth)} classes\")\n",
    "    return ground_truth\n",
    "\n",
    "def calculate_ap(retrieved, relevant, top_k=None):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    \n",
    "    relevant_set = set(relevant)\n",
    "    if top_k is not None:\n",
    "        retrieved = retrieved[:top_k]\n",
    "\n",
    "    precisions = []\n",
    "    num_correct = 0\n",
    "    \n",
    "    for i, item in enumerate(retrieved):\n",
    "        if item in relevant_set:\n",
    "            num_correct += 1\n",
    "            precision = num_correct / (i + 1)\n",
    "            precisions.append(precision)\n",
    "\n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "    \n",
    "    return sum(precisions) / min(len(relevant_set), top_k) if top_k else sum(precisions) / len(relevant_set)\n",
    "\n",
    "def evaluate_retrieval_results(retrieval_results, ground_truth, top_k=5):\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_f1 = []\n",
    "    all_ap = []\n",
    "\n",
    "    query_to_class = {}\n",
    "    with open('/Dataset/Test_element.csv', 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row) >= 2:\n",
    "                query_id = os.path.splitext(row[0])[0]\n",
    "                query_to_class[query_id] = row[1]\n",
    "    \n",
    "    print(f\"Loaded class mappings for {len(query_to_class)} queries\")\n",
    "\n",
    "    # Evaluate each query and write to CSV\n",
    "    with open('/Output/Test_map.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for query_path, results in retrieval_results.items():\n",
    "            query_id = os.path.basename(query_path).split(',')[0]\n",
    "            query_class = query_to_class.get(query_id)\n",
    "            \n",
    "            if query_class is None:\n",
    "                print(f\"Warning: No class found for query {query_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Get relevant items from ground truth\n",
    "            relevant_items = ground_truth.get(query_class, set())\n",
    "            \n",
    "            if not relevant_items:\n",
    "                print(f\"Warning: No ground truth items found for class {query_class}\")\n",
    "                continue\n",
    "\n",
    "            retrieved_ids = [os.path.basename(path).split('.')[0] for path, _ in results[:top_k]]\n",
    "\n",
    "            correct_predictions = sum(1 for item in retrieved_ids if item in relevant_items)\n",
    "\n",
    "            precision = correct_predictions / top_k if top_k > 0 else 0\n",
    "            recall = correct_predictions / len(relevant_items) if len(relevant_items) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            ap = calculate_ap(retrieved_ids, list(relevant_items), top_k=top_k)\n",
    "\n",
    "            all_precision.append(precision)\n",
    "            all_recall.append(recall)\n",
    "            all_f1.append(f1)\n",
    "            all_ap.append(ap)\n",
    "            \n",
    "            # print(f\"Query: {query_id} ({query_class})\")\n",
    "            print(f\"  AP: {ap:.4f}\")\n",
    "            print(f\"  Precision@{top_k}: {precision:.4f}\")\n",
    "            print(f\"  Recall@{top_k}: {recall:.4f}\")\n",
    "            print(f\"  F1-score@{top_k}: {f1:.4f}\")\n",
    "            writer.writerow([query_id, query_class, ap, precision, recall, f1])\n",
    "\n",
    "    avg_precision = np.mean(all_precision) if all_precision else 0\n",
    "    avg_recall = np.mean(all_recall) if all_recall else 0\n",
    "    avg_f1 = np.mean(all_f1) if all_f1 else 0\n",
    "    map_score = np.mean(all_ap) if all_ap else 0\n",
    "    \n",
    "    print(f\"\\nAverage Metrics (Top-{top_k}):\")\n",
    "    print(f\"  MAP: {map_score:.4f}\")\n",
    "    print(f\"  Precision: {avg_precision:.4f}\")\n",
    "    print(f\"  Recall: {avg_recall:.4f}\")\n",
    "    print(f\"  F1-score: {avg_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'map': map_score,\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1': avg_f1\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_config_path = \"/Weight_Path/dinov2-pytorch-base-v1\" # Pre-trained model path\n",
    "    weights_path = \"Weight_Path/dinov2_query_epoch_100.pth\" # Fine-tuned weights path\n",
    "    first_retrieval_path = \"/Initial_Retrieval/Output/first_retrieval_results.json\" # First retrieval results file\n",
    "    query_image_path = \"/home/mayunjiao/MYJ/dataset/element_all/test.txt\"  # Query image list file\n",
    "\n",
    "    correct_result_file = \"/Dataset/TestAll_index.csv\"\n",
    "    class_path = \"/Dataset/Test_element.csv\"\n",
    "    rerank_output_path = \"/Reranking/Output/reranking_results.json\"\n",
    "\n",
    "    reranking_results = perform_reranking_from_first_results(\n",
    "        model_config_path, \n",
    "        weights_path, \n",
    "        query_image_path, \n",
    "        first_retrieval_path, \n",
    "        top_k=10,\n",
    "        output_json_path=rerank_output_path\n",
    "    )\n",
    "\n",
    "    ground_truth = load_ground_truth(correct_result_file, class_path)\n",
    "\n",
    "    if reranking_results and ground_truth:\n",
    "        evaluate_retrieval_results(reranking_results, ground_truth, top_k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myj_jupyterlab",
   "language": "python",
   "name": "myj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
