{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Dinov2PreTrainedModel, Dinov2Model\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Palette images with Transparency expressed in bytes should be converted to RGBA images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sub-elements from txt file\n",
    "def load_sub_elements(txt_file):\n",
    "    sub_elements = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('.png ')\n",
    "            if len(parts) >= 2:\n",
    "                filename = parts[0]\n",
    "                # print(filename)\n",
    "                category = parts[1]\n",
    "                sub_elements.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"category\": category\n",
    "                })\n",
    "                # print(sub_elements)\n",
    "    return sub_elements\n",
    "\n",
    "# Load image database from JSON file\n",
    "def load_image_database(json_file):\n",
    "    image_db = []\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for item in data:\n",
    "            num_categories = int(len(item)/2-1)\n",
    "            # print(num_categories)\n",
    "            if len(item) >= 2:\n",
    "                filename = item[1]\n",
    "                categories = item[2: num_categories+2]\n",
    "                image_db.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"categories\": categories\n",
    "                })\n",
    "    return image_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet dataset building\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, sub_elements, image_db, sub_elements_dir, images_dir, transform=None):\n",
    "        self.sub_elements = sub_elements\n",
    "        self.image_db = image_db\n",
    "        self.sub_elements_dir = sub_elements_dir\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.category_to_images = {}\n",
    "        for img in self.image_db:\n",
    "            for cat in img[\"categories\"]:\n",
    "                if cat not in self.category_to_images:\n",
    "                    self.category_to_images[cat] = []\n",
    "                self.category_to_images[cat].append(img) ##{\"id\": {filename=Random1, categories={id1, id2, id3}}}\n",
    "\n",
    "        self.category_exclude_images = {}\n",
    "        all_image_indices = set(range(len(self.image_db)))\n",
    "        for cat in self.category_to_images:\n",
    "            cat_image_indices = {i for i, img in enumerate(self.image_db) if cat in img[\"categories\"]}\n",
    "            self.category_exclude_images[cat] = list(all_image_indices - cat_image_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sub_elements)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sub_element = self.sub_elements[idx]\n",
    "        sub_element_filename = sub_element[\"filename\"]\n",
    "        sub_element_category = sub_element[\"category\"]\n",
    "        sub_element_id = sub_element_filename.split('/')[-1].split(',')[0].strip()\n",
    "\n",
    "        sub_element_path = sub_element_filename + str('.png')\n",
    "        try:\n",
    "            sub_element_image = Image.open(sub_element_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {sub_element_path}: {e}\")\n",
    "            sub_element_image = Image.new('RGB', (224, 224))\n",
    "        \n",
    "        sub_element_image = np.array(sub_element_image)\n",
    "        positive_images = self.category_to_images.get(sub_element_id, [])\n",
    "\n",
    "        if not positive_images:\n",
    "            print(f\"Warning: No positive images found for category {sub_element_id} in image_db\")\n",
    "            positive_img = self.image_db[np.random.randint(0, len(self.image_db))]\n",
    "        else:\n",
    "            positive_img = np.random.choice(positive_images)\n",
    "            assert sub_element_id in positive_img[\"categories\"], \"Selected positive image does not contain target category\"\n",
    "\n",
    "        positive_filename = positive_img[\"filename\"]\n",
    "        positive_path = os.path.join(self.images_dir, f\"{positive_filename}.png\")\n",
    "        try:\n",
    "            positive_image = Image.open(positive_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {positive_path}: {e}\")\n",
    "            positive_image = Image.new('RGB', (224, 224))\n",
    "        \n",
    "        positive_image = np.array(positive_image)\n",
    "        negative_images = self.category_exclude_images.get(sub_element_id, [])\n",
    "        \n",
    "        if not negative_images:\n",
    "            print(f\"Warning: No negative images found for category {sub_element_id} in image_db\")\n",
    "            negative_img = self.image_db[np.random.randint(0, len(self.image_db))]\n",
    "        else:\n",
    "            negative_img = self.image_db[np.random.choice(negative_images)]\n",
    "            assert sub_element_id not in negative_img[\"categories\"], \"Selected negative image contains target category\"\n",
    "\n",
    "        negative_filename = negative_img[\"filename\"]\n",
    "        negative_path = os.path.join(self.images_dir, f\"{negative_filename}.png\")\n",
    "        try:\n",
    "            negative_image = Image.open(negative_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {negative_path}: {e}\")\n",
    "            negative_image = Image.new('RGB', (224, 224))\n",
    "        # print(sub_element, positive_filename, negative_filename)\n",
    "        negative_image = np.array(negative_image)\n",
    "        \n",
    "        # print(sub_element_id, positive_filename, negative_filename)\n",
    "        if self.transform:\n",
    "            transformed_anchor = self.transform(image=sub_element_image)\n",
    "            anchor = transformed_anchor[\"image\"]\n",
    "\n",
    "            transformed_positive = self.transform(image=positive_image)\n",
    "            positive = transformed_positive[\"image\"]\n",
    "\n",
    "            transformed_negative = self.transform(image=negative_image)\n",
    "            negative = transformed_negative[\"image\"]\n",
    "\n",
    "        anchor = torch.tensor(anchor).permute(2, 0, 1).float()\n",
    "        positive = torch.tensor(positive).permute(2, 0, 1).float()\n",
    "        negative = torch.tensor(negative).permute(2, 0, 1).float()\n",
    "        \n",
    "        return {\n",
    "            \"anchor\": anchor,\n",
    "            \"positive\": positive,\n",
    "            \"negative\": negative,\n",
    "            \"anchor_path\": sub_element_path,\n",
    "            \"positive_path\": positive_path,\n",
    "            \"negative_path\": negative_path,\n",
    "            \"category\": sub_element_category,\n",
    "            \"positive_categories\": positive_img[\"categories\"],\n",
    "            \"negative_categories\": negative_img[\"categories\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dinov2\n",
    "class Dinov2FeatureExtractor(Dinov2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dinov2 = Dinov2Model(config)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "        )\n",
    "\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self._unfreeze_dinov2_layers(2)\n",
    "        for param in self.projection_head.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def _unfreeze_dinov2_layers(self, unfreeze_layers):\n",
    "        try:\n",
    "            total_blocks = len(self.dinov2.encoder.layer)\n",
    "            layers_to_unfreeze = max(0, total_blocks - unfreeze_layers)\n",
    "            \n",
    "            print(f\"Unfreeze the last {unfreeze_layers} Transformer blocks ({layers_to_unfreeze}-{total_blocks-1})\")\n",
    "\n",
    "            for i in range(layers_to_unfreeze, total_blocks):\n",
    "                for param in self.dinov2.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"Unfreeze block {i}\")\n",
    "\n",
    "            for param in self.dinov2.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Unfreeze layernorm layer\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during unfreezing: {e}\")\n",
    "            print(\"Only train the projection head\")\n",
    "    \n",
    "    def forward(self, pixel_values, output_hidden_states=False, output_attentions=False,return_attentions=False):\n",
    "        outputs = self.dinov2(\n",
    "            pixel_values,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "        features = self.projection_head(cls_token)  # [batch_size, 256]\n",
    "        if return_attentions:\n",
    "            return features, outputs.last_hidden_state, outputs.hidden_states, outputs.attentions\n",
    "        else:\n",
    "            # return query_feat, target_feat, align_feat\n",
    "            return {\n",
    "                'features': features,\n",
    "                'last_hidden_state': outputs.last_hidden_state,\n",
    "                'hidden_states': outputs.hidden_states,\n",
    "                'attentions': outputs.attentions\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query-Guided Attention Module\n",
    "class QueryGuidedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, query_global, target_spatial):\n",
    "        \"\"\"\n",
    "        query_global: [batch_size, 1, hidden_size] sub-element features\n",
    "        target_spatial: [batch_size, seq_len, hidden_size] image features \n",
    "        \"\"\"\n",
    "        context, attn_weights = self.multihead_attn(\n",
    "            query=query_global,\n",
    "            key=target_spatial,\n",
    "            value=target_spatial,\n",
    "            need_weights=True\n",
    "        )\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Norm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "\n",
    "class AttentionFeatureExtractor(Dinov2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dinov2 = Dinov2Model(config)\n",
    "        self.cross_attention = QueryGuidedAttention(hidden_size=config.hidden_size, num_heads=8, dropout=0.1)\n",
    "\n",
    "        self.query_projection = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            L2Norm()\n",
    "        )\n",
    "        \n",
    "        self.target_projection = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            L2Norm()\n",
    "        )\n",
    "\n",
    "        self.attention_align = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 256),\n",
    "            L2Norm()\n",
    "        )\n",
    "\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self._unfreeze_dinov2_layers(2)\n",
    "\n",
    "        for param in self.query_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.target_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.attention_align.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.cross_attention.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def _unfreeze_dinov2_layers(self, unfreeze_layers):\n",
    "        try:\n",
    "            total_blocks = len(self.dinov2.encoder.layer)\n",
    "            layers_to_unfreeze = max(0, total_blocks - unfreeze_layers)\n",
    "            \n",
    "            print(f\"Unfreeze the last {unfreeze_layers} Transformer blocks ({layers_to_unfreeze}-{total_blocks-1})\")\n",
    " \n",
    "            for i in range(layers_to_unfreeze, total_blocks):\n",
    "                for param in self.dinov2.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"Unfreeze block {i}\")\n",
    "\n",
    "            for param in self.dinov2.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Unfreeze layernorm layer\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during unfreezing: {e}\")\n",
    "            print(\"Only train the projection head\")\n",
    "    \n",
    "    def forward(self, query_images, target_images, output_hidden_states=False, output_attentions=False, is_train=True, return_attentions=False):\n",
    "        query_outputs = self.dinov2(\n",
    "            query_images,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "        query_global = query_outputs.last_hidden_state[:, :1, :] # Sub-element takes the CLS token as feature\n",
    "\n",
    "        if is_train and target_images is not None:\n",
    "            target_outputs = self.dinov2(target_images)\n",
    "            target_spatial = target_outputs.last_hidden_state[:, 1:, :] # Target image takes the patch tokens as features\n",
    "\n",
    "            context, attn_weights = self.cross_attention(\n",
    "                query_global, \n",
    "                target_spatial\n",
    "            ) # query: query_global, key/value: target_spatial\n",
    "\n",
    "            query_feat = self.query_projection(query_global.squeeze(1))\n",
    "            target_feat = self.target_projection(context.squeeze(1))\n",
    "            align_feat = self.attention_align(query_global.squeeze(1)) \n",
    "\n",
    "            if return_attentions:\n",
    "                return query_feat, target_feat, align_feat, attn_weights\n",
    "            else:\n",
    "                return query_feat, target_feat, align_feat\n",
    "        else:\n",
    "            return self.query_projection(query_global.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.2):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.distance_fn = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_sim = self.distance_fn(anchor, positive)\n",
    "        neg_sim = self.distance_fn(anchor, negative)\n",
    "\n",
    "        loss = torch.mean(torch.clamp(self.margin - pos_sim + neg_sim, min=0.0))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Data path\n",
    "    sub_elements_txt = '/Dataset/train_sub.txt'  # Path and category of sub elements training dataset\n",
    "    val_sub_txt = '/Dataset/val_sub.txt' # Path and category of sub elements validating dataset\n",
    "    image_db_json = '/Dataset/Train_split.json'  # Image database for training\n",
    "    val_image_db = '/Dataset/Validation_split.json' # Image database for validating\n",
    "    sub_elements_dir = '/Dataset/element img'  # Sub-element images directory\n",
    "    images_dir = '/Dataset/SimulatedPrintedFabrics-17k/train/img/images'  # Images for training\n",
    "    val_dir = '/Dataset/SimulatedPrintedFabrics-17k/validation/img/images' # Images for validating\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading sub-elements...\")\n",
    "    sub_elements = load_sub_elements(sub_elements_txt)\n",
    "    val_sub = load_sub_elements(val_sub_txt)\n",
    "    print(f\"Loaded {len(sub_elements)} sub-elements\")\n",
    "    \n",
    "    print(\"Loading image database...\")\n",
    "    image_db = load_image_database(image_db_json)\n",
    "    val_db = load_image_database(val_image_db)\n",
    "    print(f\"Loaded {len(image_db)} images\")\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_size = len(sub_elements)\n",
    "    val_size = len(val_sub)\n",
    "    print(train_size, val_size)\n",
    "    train_sub_elements = sub_elements\n",
    "    val_sub_elements = val_sub\n",
    "    \n",
    "    train_dataset = TripletDataset(train_sub_elements, image_db, sub_elements_dir, images_dir, train_transform)\n",
    "    val_dataset = TripletDataset(val_sub_elements, val_db, sub_elements_dir, val_dir, val_transform)\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        anchor_batch = torch.stack([item[\"anchor\"] for item in batch])\n",
    "        positive_batch = torch.stack([item[\"positive\"] for item in batch])\n",
    "        negative_batch = torch.stack([item[\"negative\"] for item in batch])\n",
    "        \n",
    "        return {\n",
    "            \"anchor\": anchor_batch,\n",
    "            \"positive\": positive_batch,\n",
    "            \"negative\": negative_batch,\n",
    "            \"anchor_paths\": [item[\"anchor_path\"] for item in batch],\n",
    "            \"positive_paths\": [item[\"positive_path\"] for item in batch],\n",
    "            \"negative_paths\": [item[\"negative_path\"] for item in batch],\n",
    "            \"categories\": [item[\"category\"] for item in batch]\n",
    "        }\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=6, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "    \n",
    "    # Model initialization\n",
    "    model = AttentionFeatureExtractor.from_pretrained(\"/Weight_Path/dinov2-pytorch-base-v1\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Definition of loss functions\n",
    "    triplet_criterion = TripletLoss(margin=0.2)\n",
    "    \n",
    "    lr = 1e-5\n",
    "    params = [\n",
    "        {'params': [p for n, p in model.named_parameters() if 'dinov2' in n and p.requires_grad], 'lr': lr/10},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'query_projection' in n], 'lr': lr},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'target_projection' in n], 'lr': lr},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'attention_align' in n], 'lr': lr},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'cross_attention' in n], 'lr': lr*2}  \n",
    "    ]\n",
    "    feature_optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=5e-4)\n",
    "    feature_scheduler = CosineAnnealingLR(feature_optimizer, T_max=80, eta_min=lr/100)\n",
    "    \n",
    "    # Training function\n",
    "    def train_one_epoch(feature_extractor, dataloader, triplet_criterion, feature_optimizer, device):\n",
    "        feature_extractor.train()\n",
    "        total_triplet_loss = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            anchor = batch[\"anchor\"].to(device)\n",
    "            positive = batch[\"positive\"].to(device)\n",
    "            negative = batch[\"negative\"].to(device)\n",
    "\n",
    "            feature_optimizer.zero_grad()\n",
    "\n",
    "            # Sub-element feature extraction\n",
    "            anchor_features = feature_extractor(query_images=anchor, target_images=None)[0]\n",
    "            \n",
    "            # Positive feature extraction (using anchor to query positive image)\n",
    "            _, positive_features, _ = feature_extractor(\n",
    "                query_images=anchor,\n",
    "                target_images=positive\n",
    "            )\n",
    "            \n",
    "            # Negative feature extraction (using anchor to query negative image)\n",
    "            _, negative_features, _ = feature_extractor(\n",
    "                query_images=anchor,\n",
    "                target_images=negative\n",
    "            )\n",
    "\n",
    "            triplet_loss = triplet_criterion(anchor_features, positive_features, negative_features)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            triplet_loss.backward()\n",
    "            feature_optimizer.step()\n",
    "            \n",
    "            total_triplet_loss += triplet_loss.item()\n",
    "            \n",
    "        return total_triplet_loss / len(dataloader)\n",
    "    \n",
    "    # Evaluation function\n",
    "    def evaluate(feature_extractor, dataloader, triplet_criterion, device):\n",
    "        feature_extractor.eval()\n",
    "\n",
    "        total_triplet_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                anchor = batch[\"anchor\"].to(device)\n",
    "                positive = batch[\"positive\"].to(device)\n",
    "                negative = batch[\"negative\"].to(device)\n",
    "                \n",
    "                # Sub-element feature extraction\n",
    "                anchor_features = feature_extractor(query_images=anchor, target_images=None)[0]\n",
    "            \n",
    "                # Positive feature extraction (using anchor to query positive image)\n",
    "                _, positive_features, _ = feature_extractor(\n",
    "                    query_images=anchor,\n",
    "                    target_images=positive\n",
    "                )\n",
    "                \n",
    "                # Negative feature extraction (using anchor to query negative image)\n",
    "                _, negative_features, _ = feature_extractor(\n",
    "                    query_images=anchor,\n",
    "                    target_images=negative\n",
    "                )\n",
    "                \n",
    "                # Loss computation\n",
    "                triplet_loss = triplet_criterion(anchor_features, positive_features, negative_features)\n",
    "                total_triplet_loss += triplet_loss.item()\n",
    "        \n",
    "        return total_triplet_loss / len(dataloader)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    print('Starting training...')\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_triplet_loss = train_one_epoch(\n",
    "            model, train_dataloader, \n",
    "            triplet_criterion,\n",
    "            feature_optimizer, device\n",
    "        )\n",
    "        \n",
    "        val_triplet_loss = evaluate(\n",
    "            model, val_dataloader, \n",
    "            triplet_criterion, device\n",
    "        )\n",
    "\n",
    "        feature_scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: Triplet={train_triplet_loss:.4f}\")\n",
    "        print(f\"Val Loss:   Triplet={val_triplet_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'feature_extractor_state_dict': model.state_dict(),\n",
    "                'feature_optimizer_state_dict': feature_optimizer.state_dict(),\n",
    "                'train_triplet_loss': train_triplet_loss,\n",
    "                'val_triplet_loss': val_triplet_loss,\n",
    "            }, f'fine_tuned/dinov2_query_epoch_{epoch+1}.pth')\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myj_jupyterlab",
   "language": "python",
   "name": "myj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
